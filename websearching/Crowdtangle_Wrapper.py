#!/usr/bin/env python
# coding: utf-8

## Code used to search for posts on Facebook, identify links and images, anonymise and deduplicate results ##
## Created by Bronwen Hunter on 10th October 2021 ##
##Â Last edited by Bronwen Hunter on 05/01/2022 ##

# Library imports 
import pandas as pd
import requests
import json
import time
import datetime
import argparse
import re
import numpy as np
import os

class crowdtangle_wrapper():

    '''Class that packages up functions needed to search Crowdtangle to Facebook posts'''
    
    def __init__(self,
                 querypath,              # File where list of queries is stored
                 api_token,              # API token associated with crowdtangle account
                 filepath,               # Directory where outputs will be stored
                 resulttype,             # Type of posts to be retrieved
                 start_date,             # Date to start the search 
                 numberofyears,          # Number of years to search after the start date
                 language = 'en',
                 lists = 'NA',           # (if applicable) the Crowdtangle lists to search 
                 searchidentifier='FB'): # String to append to anonymous id
        
        df = pd.read_csv(querypath)
        self.listofqueries = list(df['queries'])
        self.api_token = api_token
        self.filepath = filepath
        self.resulttype = resulttype
        self.start_date = start_date
        self.numberofyears = numberofyears
        self.language = language
        self.lists = lists
        self.searchidentifier = searchidentifier
        
        
    def search_crowdtangle(self):
        
        ''' Main function to search Facebook using the crowtangle API'''

        os.mkdir(self.filepath + 'queries')
    
        if self.language == 'NA':
            language = None
        else:
            language = self.language

        if self.lists == 'NA':
            list1 = None
        else:
            listdf = pd.read_csv(self.lists)
            list1 = ",".join(list(listdf['list_id']))
        search_url = "https://api.crowdtangle.com/posts/search"
        querydict = {}
        dates = (crowdtangle_wrapper.date_range(self))
        for query in self.listofqueries:
            datedict = {}
            for date in dates:
                pagedict = {}
                i = 0
                if i == 0:
                    PARAMS = {'searchTerm': query,
                              'count':100,
                              'token':self.api_token,
                              'startDate':date[0],
                              'endDate':date[1],
                              'types': self.resulttype,
                              'sortBy':'date',
                              'offset':0,
                              'platforms':'facebook',            # Only search within Facebook
                              'searchField':'text_fields_only',  # Only search for keywords within text fields
                              'language':language,
                              'inListIds':list1}
                    response = requests.get(search_url, params=PARAMS)
                    time.sleep(10)                               # Required to not exceed API rate limit
                    if response.status_code != 200:
                        raise Exception(response.status_code, response.text)
                    search_results = response.json()
                    search_results = search_results['result']['posts']
                    if len(search_results)>0:
                        pagedict[i] = search_results
                    print("Obtained page {} from search {} {}-{}".format(i+1, query, date[0], date[1]))
                    i +=1
                    time.sleep(11)
                while i>0:
                    PARAMS = {'searchTerm': query,
                              'count':100,
                              'token':self.api_token,
                              'startDate':date[0],
                              'endDate':date[1],
                              'types': self.resulttype,
                              'sortBy':'date',
                              'offset':i*100,
                              'platforms':'facebook',
                              'searchField':'text_fields_only',
                              'language':language,
                              'inListIds':list1}
            
                    response = requests.get(search_url, params=PARAMS)
                    time.sleep(10)
                    if response.status_code != 200:
                        raise Exception(response.status_code, response.text)
                    search_results = response.json()
                    search_results = search_results['result']['posts']
                    if len(search_results)>0:
                        pagedict[i] =search_results
                        print("Obtained page {} from search {} {}-{}".format(i+1, query, date[0], date[1]))
                        i += 1
                    else:
                        break
                datedict[str(date)] = pagedict # Add the json response to a dictionary 
            querydict[query] = crowdtangle_wrapper.create_df(self, datedict, query) # Create dataframe from dictionary
        
        finaldf = pd.concat(querydict.values(), ignore_index = True)             # Concatenate all results
        deduplicated_df = finaldf.drop_duplicates(subset = ['id']).reset_index() # Removing duplicate posts
        df_anon = crowdtangle_wrapper.anonymise_fb(self, deduplicated_df)       # Replace post ids with anonymous ids
        df_regex = fill_gaps_regex(df_anon)                                     # Identify missing URLs using regex
        df_regex.to_csv(self.filepath+"facebook_allqueries_{}_{}.csv".format(dates[0][0], dates[-1][1]), escapechar="\r")
        return df_regex
    
    
    def create_df(self, datedict, query):
        
        ''' Creat dataframe from Facebook search results'''
    
        dflist = {}
        for date, pages in datedict.items():      # Opening up each date dictionary
            for page, results in pages.items():   # Opening up each page dictionary 
                df = pd.DataFrame(columns = ['query', 'id', 'date', 'post_text', 'external_url', 'image_url'])
                df['query'] = [query for i in range(len(results))]
                df['id'] = [results[i]['id'] for i in range(len(results))]
                df['date'] = [results[i]['date'] for i in range(len(results))]
                df['post_text'] = [results[i]['message'] if 'message' in results[i].keys() else np.nan for i in range(len(results))]
                for i in range(len(results)):
                    urls = []
                    images = []
                    try:
                        for item in range(len(results[i]['expandedLinks'])):
                            if 'photo' in results[i]['expandedLinks'][item]['original']:
                                if 'facebook.com' in results[i]['expandedLinks'][item]['original']:    # Identifying which URLs are photos 
                                    if results[i]['expandedLinks'][item]['original'] not in images:
                                            images.append(results[i]['expandedLinks'][item]['original'])
                            elif 'facebook.com' not in results[i]['expandedLinks'][item]['expanded']:  # Identifying external URLs
                                if results[i]['expandedLinks'][0]['expanded'] not in urls:
                                    urls.append(results[i]['expandedLinks'][0]['expanded'])
                    except: False
                    df.loc[i, 'external_url'] = ",".join(urls) # Joining all URLs so they can fit in one df cell
                    df.loc[i, 'image_url'] = ",".join(images)  # Joining all image URLs so they can fit in one df cell
                dflist[str(date)+str(page)] = df
        if len(dflist.values())>0:
            finaldf = pd.concat(dflist.values(), ignore_index = True)
            finaldf.to_csv(self.filepath + 'queries/' + 'Facebook_query_{}_{}.csv'.format(query, self.start_date), escapechar="\r")
            return finaldf
        else:
            print("No results were obtained for this query {}".format(query))
    
    
    def date_range(self):
        ''' Gets date range with defined start, end and interval '''
        
        from datetime import datetime, timedelta
        ranges = []
        years = self.numberofyears 
        start = datetime.strptime(self.start_date,"%Y-%m-%d")
        for i in range(years):
            ranges.append(((start + timedelta(days=365*i)).strftime("%Y-%m-%d"), (start + timedelta(days = 365*(i+1))).strftime("%Y-%m-%d")))
        return ranges
  

    def anonymise_fb(self, df):
        
        ''' Anonymise facebook post IDs and save a key document'''

        keydf = pd.DataFrame()
        keydf['facebook_id'] = [df.loc[i, 'id'] for i in df.index]
        keydf['random_id'] = [self.searchidentifier+self.start_date[2:4]+ str(i) for i in df.index]
        df['random_id'] = [self.searchidentifier+self.start_date[2:4]+ str(i) for i in df.index]
        keydf.to_csv(self.filepath+"facebook_id_key_{}.csv".format(self.start_date), escapechar="\r") # Saving key document
        return df
    
def Find(string):
    
    '''Find urls in a string'''

    url = re.findall(r'(https?://[^\s]+)',string, flags=re.UNICODE)      
    return [x[0] for x in url]


def fill_gaps_regex(df):

    '''Fill in datagaps where twitter API has not recognised urls'''

    df = df
    for i in range(len(df)):
        print(i)
        if len(str(df.loc[i, 'external_url'])) <4:
            df.loc[i, 'external_url']=",".join(Find(str(df.loc[i, 'post_text'])))
    
    return df

 
def main():

    '''The main function defines the interface with the users.'''

    parser = argparse.ArgumentParser()
    parser.add_argument("--queryfile", default=None, required=True, type=str, help="File where the queries are stored")
    parser.add_argument("--api_token", default=None, required=True, type=str, help='Crowdtangle api token')
    parser.add_argument("--output", default = None, required=True, type=str, help='Directory where output files will be saved')
    parser.add_argument("--post_type", default = 'link,photo', type=str, help='Type of facebook posts to be returned')
    parser.add_argument("--start_date", default = '2010-01-01', type=str, help= 'Search start date (YYYY-MM-DD)')
    parser.add_argument("--numberofyears", default='1',type = int, help='number of years after start date to search')
    parser.add_argument("--lang", default='en', type=str, help='Language of search results')
    parser.add_argument("--listdf", default = 'NA', type=str, help="Dataframe where list id's are stored")
    parser.add_argument("--anonid", type=str, default='FB', help='string to append to random id for anonymisation')
    
    args = parser.parse_args()
    inst = crowdtangle_wrapper(args.queryfile, 
                               args.api_token,
                               args.output,
                               args.post_type,
                               args.start_date,
                               args.numberofyears,
                               args.lang,
                               args.listdf,
                               args.anonid)
    inst.search_crowdtangle()
    


if __name__ == "__main__":
    main()

