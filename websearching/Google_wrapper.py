#!/usr/bin/env python
# coding: utf-8

## Code used to interact with the Google API to return web results using search queries ##
## Last edited by Bronwen Hunter on 05/01/2022 ##


import os
import traceback
import sys
import pandas as pd
import time,requests,json, math
import numpy as np
import math
from datetime import datetime
import argparse

class google_wrapper():

    ''' Class that packages up functions needed to search Google '''
    
    def __init__(self, 
                 queryfile, # File where search queries are stored
                 filepath,  # Directory where output files will be saved
                 apikey,    # API key associated with Google cloud services account 
                 filetype,  # The type of files to return in search results 
                 number_of_results = 100, # The max number of results per query is 100
                 dateyears = 20, # The number of years across which to start (working back from today)
                 language = 'en'):
        
        df = pd.read_csv(queryfile)
        self.filepath = filepath
        self.querylist = list(df['queries'])
        self.apikey = apikey
        self.filetype = filetype
        self.number_of_results = number_of_results
        self.dateyears = dateyears
        self.language = language
        
        
    def google_search(self):

        ''' Function to programatically access Google search results - note only 100 results per query can be brought back'''
        
        search_url = 'https://www.googleapis.com/customsearch/v1?'
        results_dict = {}
        numberofpages = int(self.number_of_results/10) # Can only retrieve search results 10 per page
        now = datetime.now()
        dt_string = now.strftime("%d-%m-%Y")

        for query in self.querylist:
            pagedict = {}
            for page in range(numberofpages):
                results_df = pd.DataFrame() # Creating a new dataframe for each page
                response=requests.get(search_url, params = {'cx':'d031fa5af8a9d481d',
                                                                'key':self.apikey,
                                                                'q':query,
                                                                'fileType':self.filetype,
                                                                'safe': 'active',
                                                                'lr':self.language,
                                                                'dateRestrict':'y['+str(self.dateyears)+']',
                                                                'start':(page*10)+1})
                if response.status_code != 200: # If the response doesn't work, wait 5 minutes and try again
                    time.sleep(300)
                    response=requests.get(search_url, params = {'cx':'d031fa5af8a9d481d',
                                                                'key':self.apikey,
                                                                'q':query,
                                                                'fileType':self.filetype,
                                                                'safe': 'active',
                                                                'lr':self.language,
                                                                'dateRestrict':'y['+str(self.dateyears)+']',
                                                                'start':(page*10)+1})
                response = response.json()
                print("{} : Documents on the page {}/{},are being extracted".format(query, page, 10))
                time.sleep(10)
                if 'items' in response.keys():
                    results_df['title'] = [response['items'][i]['title'] for i in range(len(response['items']))]
                    results_df['external_url']=[response['items'][i].get('link') for i in range(len(response['items']))]
                    results_df['query'] = [query for item in results_df['title']]
                    results_df['source'] = ['google' for item in results_df['title']]
                    pagedict[page] = results_df
            if len(pagedict.values())>0:
                querydf = pd.concat(pagedict.values(), ignore_index = True) # Concatenating all dataframes from the dictionary 
                querydf.to_csv(self.filepath + 'query_{}_{}.csv'.format(query, dt_string))
                results_dict[query] = querydf
        dataframe = pd.concat(results_dict.values(), ignore_index = True) # Concatenating results from all queries
        dataframe2 = dataframe.drop_duplicates(subset = 'external_url').reset_index()  # Deduplicating by results URL

        dataframe2['id'] = [('G'+str(i)) for i in range(len(dataframe2))] # Creating random id for each result
        dataframe2['search_date'] = [dt_string for i in range(len(dataframe2))]
        dataframe2.to_csv(self.filepath + 'googlesearch_allqueries_{}.csv'.format(dt_string))
        return dataframe2
        
def main():
    
    """The main function defines the interface with the users."""
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--queryfile', required=True, default=None, type=str, help='File where queries are stored')
    parser.add_argument('--filepath', required=True, default=None, type=str, help='Directory where function output will be stored')
    parser.add_argument('--api_key', required=True, default=None, type=str, help='Google custom search API key')
    parser.add_argument('--filetype', required=False, default='html', type=str, help='File type of search results')
    parser.add_argument('--timeperiod', required=False, default=1, type=int, help='Number of years back to search')
    parser.add_argument('--numberofresults', required=False, default=1, type=int, help='Number of results to bring back')
    parser.add_argument('--lang', required=False, default='en', type=str, help='Language of results')
    
    args = parser.parse_args()
    
    inst = google_wrapper(args.queryfile,
                         args.filepath,
                         args.api_key,
                         args.filetype,
                         args.numberofresults,
                         args.timeperiod,
                         args.lang)
    inst.google_search()
    

if __name__ == "__main__":
    main()





