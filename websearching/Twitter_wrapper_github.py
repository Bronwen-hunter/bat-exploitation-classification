 #!/usr/bin/env python
# -*- coding: utf-8 -*-

## Code to programatically access posts from Twitter (can either search for users or keywords) ##
## Created by Bronwen Hunter on 08/10/21 ##
## Last edited by Bronwen Hunter on 05/01/2022 ##

import requests
import os
import json
from requests_oauthlib import OAuth1Session
import math
import pandas as pd
from datetime import date, datetime
import time
import numpy as np
from itertools import islice
import re
import argparse


class twitter_wrapper():
    
    ''' Class that can be used to collect data from the twitter v2 API'''
    
    def __init__(self,
                 queryfile,         # File where the queries are stored
                 directory,         # Directory where outputs will be saved
                 users = False,     # Are we searching for specific users?
                 number_of_results=0, # Number of results to return (if 0 will return all results)
                 page_size=500,       # Max number of results per page is 500
                 language='NA',       # The language of tweets you want to return
                 start_time='2006-05-26T00:00:00Z', # Start time (tweets will be collected in date order, newest first)
                 end_time='2021-10-07T00:00:00Z',   # End time
                 identifier = 'T'):                 # Used to anonymise the tweet dataset
        df = pd.read_csv(queryfile)
        self.querylist=list(df['queries'])
        self.directory=directory
        self.number_of_results=number_of_results
        self.page_size=page_size
        self.language=language
        self.start_time=start_time
        self.end_time=end_time
        self.search_url = "https://api.twitter.com/2/tweets/search/all"
        self.lookup_url = "https://api.twitter.com/2/tweets"
        self.users=users
        self.identifier = identifier
    
    def retrieve_search(self):
        
        ''' Function to retrieve all queries from a list in a given timeframe'''

        os.mkdir(self.directory + 'queries')
        start = time.process_time()
        resultsdict={}
        for query in self.querylist:
            print(query)
            if self.language=='NA':
                query1=query
            else:
                query1=query+' lang:'+self.language
            
            pagedict={}
            i = 0
            if self.number_of_results == 0:
                if i == 0:
                    response = connect_to_endpoint({'query':query1,
                                                       'tweet.fields':'created_at,entities,text,referenced_tweets',
                                                       "max_results":self.page_size,
                                                       'start_time':self.start_time,
                                                       'end_time':self.end_time})
                    time.sleep(6)
                    pagedict[i] = response
                    print("{}: downloaded page {}".format(query, i+1))
                    i += 1
                    
                while i>0:    
                    if'next_token' in pagedict[i-1]['meta'].keys():
                         response = connect_to_endpoint({'query':query1,
                                                       'tweet.fields':'created_at,entities,text,referenced_tweets',
                                                       'max_results':self.page_size,
                                                       'start_time':self.start_time,
                                                       'end_time':self.end_time,
                                                       'next_token':pagedict[i-1]['meta']['next_token']})
                         time.sleep(6)
                         if 'data' in response.keys():
                             pagedict[i] = response
                         else: break                    # Breaking when no more results are returned 
                         print("{}: downloaded page {}".format(query, i+1))
                         i += 1
                         
                    else:
                        break
            else:
                numberofpages = math.ceil(self.number_of_results/self.page_size) # Otherwise calculating the number of pages to return
                for i in range(0, numberofpages):
                    if i == 0:
                        response = connect_to_endpoint({'query':query1,
                                                       'tweet.fields':'created_at,entities,text,referenced_tweets',
                                                       "max_results":self.page_size,
                                                       'start_time':self.start_time,
                                                       'end_time':self.end_time
                                                   })
                        time.sleep(6)
                        pagedict[i] = response
                    else:
                        response = connect_to_endpoint({'query':query1,
                                                       'tweet.fields':'created_at,entities,text,referenced_tweets',
                                                       'max_results':self.page_size,
                                                       'start_time':self.start_time,
                                                       'end_time':self.end_time,
                                                       'next_token':pagedict[i-1]['meta']['next_token']})
                        time.sleep(6)
                        if 'data' in response.keys():
                            pagedict[i] = response
            if 'data' in pagedict[0].keys():
                df=twitter_wrapper.convert_to_df(self, query, pagedict)
                retweetlist={}
                if 'retweet_id' in df.columns:
                    for j, tweet in enumerate(df['retweet_id']):
                        if len(str(tweet))>3:
                            retweetlist[j] = tweet
                    for dictionary in chunks(retweetlist):
                        ids = ','.join(list(dictionary.values()))
                        response = requests.get(self.lookup_url,
                                                    auth=bearer_oauth,
                                                    params = {"ids":ids,'tweet.fields':'created_at,text,entities'}).json()
                        print('Retrieved retweets {}'.format(ids))
                        time.sleep(6)
                if 'data' in response.keys():
                    for item in range(len(response['data'])):
                        for index, ID in retweetlist.items():
                            if response['data'][item]['id'] == ID:
                                df.loc[index, 'retweet_text'] = response['data'][item]['text']
                                if 'entities' in response['data'][item].keys():
                                    if 'urls' in response['data'][item]['entities'].keys():
                                        external_urls = []
                                        image_urls = []
                                        for k, url in enumerate(response['data'][item]['entities']['urls']):
                                            if 'twitter.com' not in response['data'][item]['entities']['urls'][k]['expanded_url']:
                                                external_urls.append(response['data'][item]['entities']['urls'][k]['expanded_url'])
                                            elif 'pic.twitter.com' in response['data'][item]['entities']['urls'][k]['display_url']:
                                                image_urls.append(response['data'][item]['entities']['urls'][k]['expanded_url'])
                                        if len(external_urls)>0:
                                            df.loc[index, 'retweet_url'] = ",".join(external_urls)
                                        if len(image_urls)>0:
                                            df.loc[index, 'retweet_image'] = ",".join(image_urls)
                df.to_csv(self.directory + "queries/"+ "twitter_query_{}_{}.csv".format(query, self.start_time[:10]))
                resultsdict[query] = df
        df_joined = twitter_wrapper.aggregate_twitter(self, resultsdict)
        df_joined = fill_gaps_regex(df_joined)
        print(df_joined)
        df_anon = twitter_wrapper.anonymise_twitter(self, df_joined)
        df_anon.to_csv(self.directory+'twitter_allqueries_{}_{}.csv'.format(self.start_time[:10], self.end_time[:10]))
        return  df_anon

    
    def convert_to_df(self, query, dictionary):
        
        '''Convert a dictionary of results to a dataframe'''
    
        df_dict = {}
        for page in dictionary.keys():
            df = pd.DataFrame(columns = ['id', 'date', 'post_text', 'external_url', 'image_url',
            'retweet_id', 'retweet_text', 'retweet_url', 'retweet_image'])
            if 'data' in dictionary[page].keys():
                data = dictionary[page]['data']
                df['id'] = [data[i]['id'] for i, tweet in enumerate(data)]
                df['date']= [data[i]['created_at'] for i, tweet in enumerate(data)]
                df['post_text'] = [data[i]['text'] for i, tweet in enumerate(data)]
                df['retweet_id'] = [data[i]['referenced_tweets'][0]['id'] if 'referenced_tweets' in data[i].keys() else np.nan for i, tweet in enumerate(data) ]
                for i, tweet in enumerate(data):    
                    if 'entities' in data[i].keys():
                        if 'urls' in data[i]['entities'].keys():
                            image_urls = []
                            external_urls = []
                            for j, item in enumerate(data[i]['entities']['urls']):
                                if 'pic.twitter.com' in data[i]['entities']['urls'][j]['display_url']:
                                    image_urls.append(data[i]['entities']['urls'][j]['expanded_url'])
                                elif 'twitter.com' not in data[i]['entities']['urls'][j]['expanded_url']:
                                    external_urls.append(data[i]['entities']['urls'][j]['expanded_url'])
                            if len(image_urls)>0:
                                df.loc[i, 'image_url'] = ",".join(image_urls)
                            if len(external_urls)>0:
                                df.loc[i, 'external_url'] = ",".join(external_urls)
                df_dict[page] = df 
        try:
            df_all = pd.concat(df_dict.values(), ignore_index = True)
            return df_all
        except:
             print("No results obtained from {}".format(query))
    
    def aggregate_twitter(self, resultsdict):
        
        '''Aggregate a number of dataframes and deduplicate'''
    
        for query, df in resultsdict.items():
            df['query'] = [query for i in range(len(df))]
        df_joined = pd.concat(resultsdict.values(), ignore_index = True)
        df_joined = df_joined.drop_duplicates(subset = ['id']).reset_index()
        df_joined = df_joined.drop_duplicates(subset = ['post_text']).reset_index()
        return df_joined
    
    def anonymise_twitter(self, df):
        
        '''Replace tweet ids with a random identifier and save a key document'''

        keydf = pd.DataFrame()
        if self.users == 'True':
            additional = 'U'
        else:
            additional=''
        keydf['tweet_id'] = [df.loc[i, 'id'] for i in df.index]
        keydf['random_id'] = [self.identifier + additional + str(self.end_time)[2:4] + str(i)for i in df.index]
        df['id'] = [self.identifier + additional + str(self.end_time)[2:4] + str(i)for i in df.index]
        keydf.to_csv(self.directory+"tweet_id_key_{}.csv".format(self.end_time[:10]))
        return df

def chunks(data, SIZE=30):
    
    '''Split up a dictionary into chunks'''
    
    it = iter(data)
    for i in range(0, len(data), SIZE):
        yield {k:data[k] for k in islice(it, SIZE)}

        
def Find(string):
    
    '''Find urls in a string'''
    
    regex = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"
    url = re.findall(regex,string, flags=re.UNICODE)      
    return [x[0] for x in url]


def fill_gaps_regex(df):

    '''Fill in datagaps where twitter API has not recognised urls'''

    df = df
    for i in range(len(df)):
        if len(str(df.loc[i, 'external_url'])) <4:
            df.loc[i, 'external_url']=",".join(Find(str(df.loc[i, 'post_text'])))

        if len(str(df.loc[i, 'retweet_url'])) <4:
            if len(str(df.loc[i, 'retweet_id'])) >3:
                df.loc[i, 'retweet_url']=",".join(Find(str(df.loc[i, 'retweet_text'])))

    return df

bearer_token = "" # Here you need to put your bearer token

def bearer_oauth(r):

    ''' API authorisation function '''

    r.headers["Authorization"] = "Bearer {}".format(bearer_token)
    r.headers["User-Agent"] = "v2FullArchiveSearchPython"
    return r

def connect_to_endpoint(params):

     '''Function to connect to Twitter API endpoint'''

     search_url = "https://api.twitter.com/2/tweets/search/all"
     response = requests.request("GET", search_url, auth=bearer_oauth, params=params)
     if response.status_code != 200:
         raise Exception(response.status_code, response.text)
     return response.json()
              


def main():
    ''' Main function that determines interaction with the users'''
    parser = argparse.ArgumentParser()
    parser.add_argument('--queryfile', type=str, default=None, required=True, help="File where queries are saved.")
    parser.add_argument('--users', type=str, default='False', required=True, help="Define whether you are searching for users or keywords")
    parser.add_argument('--output', default=None, type=str, required=True, help='Directory where output will be saved')
    parser.add_argument('--numberofresults', default=0, type=int, required=False, help='Number of results to bring back')
    parser.add_argument('--pagesize', default=500, type=int, required=False, help='Number of results per page')
    parser.add_argument('--lang', default='NA', type=str, required=False, help='language of posts to be returned')
    parser.add_argument('--start_time', default='2006-05-26T00:00:00Z', type=str, required=False, help='Start date of search: YYYY-MM-DDTHH:MM:SSZ')
    parser.add_argument('--end_time', default='2021-10-10T00:00:00Z', type=str, required=False, help='End date of search: YYYY-MM-DDTHH:MM:SSZ')
    parser.add_argument('--identifier', defualt = 'T', type=str, required=False, help='identifier used to anonymise results')
    
    args = parser.parse_args()
    print(args)

    inst=twitter_wrapper(args.queryfile,
                        args.output,
                        args.users,
                        number_of_results = int(args.numberofresults),
                        page_size = int(args.pagesize),
                        language = args.lang,
                        start_time=args.start_time,
                        end_time = args.end_time,
                        identifier=args.identifier)
    
    inst.retrieve_search()
    
        

if __name__ == "__main__":
    main()

