##Â Functions used to generate chunks to be used in BERT classification ##

from nltk import sent_tokenize, ToktokTokenizer
import pandas as pd
from itertools import chain


def get_split_text(text, length):
    
  ''' Function to split text (by whitespace) into chunks of equal length'''

  text = text.split()
  chunks = [" ".join(text[i:i+length]) for i in range(0, len(text), length)]
  return chunks


def tok_tokenize(string):

  ''' Tokenizing text using the nltk ToktokTokenizer'''  

  toktok = ToktokTokenizer()
  tokenized_corpus = [toktok.tokenize(sent) for sent in sent_tokenize(string)]
  tokenized_corpus = list(chain(*tokenized_corpus))
  return tokenized_corpus

def generate_bert_chunk(text, keywords1, keywords2, keywords3, maxlength):

  ''' Main function to generate chunks for BERT classification '''
    
  text = text.replace('\n', ' ')
  keyword_match2 = []
  if len(text.split()) > maxlength:
    chunks = get_split_text(text, maxlength)
    chunklist = [tok_tokenize(chunk.lower()) for chunk in chunks]
    keyword_match =[i for i in range(len(chunks)) if any(word in chunklist[i] for word in keywords1)]
    if len(keyword_match)>1:
      keyword_match2 = [i for i in keyword_match if any(word2 in chunklist[i] for word2 in keywords2)]
    if len(keyword_match2) <1:
      keyword_match2 = [i for i in keyword_match if any(word2 in chunklist[i] for word2 in keywords3)]
    if len(keyword_match2)>0:
      return chunks[keyword_match2[0]]
    elif len(keyword_match) >0:
      return chunks[keyword_match[0]]
    else:
      return chunks[0]
  else:
    return text    
