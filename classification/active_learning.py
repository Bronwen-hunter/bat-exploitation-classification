## Functions to use the smalltext library to perform active learning with BERT ##

# Importing all the query strategies and library requirements

from small_text.query_strategies import EmbeddingKMeans
from small_text.query_strategies import LeastConfidence
from small_text.integrations.transformers.datasets import TransformersDataset

from small_text.active_learner import PoolBasedActiveLearner
from small_text.initialization import random_initialization
from small_text.integrations.transformers import TransformerModelArguments
from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory
from small_text.integrations.transformers import TransformerModelArguments
from keras.backend import clear_session

import logging
import pandas as pd
import transformers
from transformers import BertTokenizerFast
from sklearn.metrics import accuracy_score, precision_recall_fscore_support


def get_transformers_dataset(tokenizer, data, labels, max_length=512):
    
  ''' Function to create a dataset for transformers classification'''

  data_out = []
  for i, doc in enumerate(data):
    encoded_dict = tokenizer.encode_plus(
            doc,
            add_special_tokens=True,
            padding='max_length',
            max_length=max_length,
            return_attention_mask=True,
            return_tensors='pt',
            truncation='longest_first'
        )
    data_out.append((encoded_dict['input_ids'], encoded_dict['attention_mask'], labels[i]))
  return TransformersDataset(data_out)


def initialize_active_learner(active_learner, y_train, numberofsamples=10):

    ''' Simulates an initial labeling to warm-start the active learning process '''

    x_indices_initial = random_initialization(y_train, n_samples=numberofsamples)
    y_initial = y_train[x_indices_initial]

    active_learner.initialize_data(x_indices_initial, y_initial)

    return x_indices_initial


def evaluate(active_learner,test):

    ''' Evaluating the performance of the model'''

    y_pred_test = active_learner.classifier.predict(test)
    test_acc = accuracy_score(y_pred_test, test.y)
    precision, recall, fscore, support=precision_recall_fscore_support(test.y, y_pred_test, average='binary', pos_label=1)
    print('Test accuracy: {:.2f}'.format(test_acc))
    return (test_acc, precision, recall, fscore, support)

def save_active_leaner(active_learner, filepath, iteration, query):
  filepath = filepath + "active_learner_{}_{}.pkl".format(iteration, query)
  active_learner.save(filepath)
  print("active_learner_{}.pkl".format(iteration))

def fullpipeline(train_idx, 
                 train_text,
                 test_idx,
                 test_text,
                 querysize,
                 numberofqueries,
                 query_strategy,
                 transformer_model,
                 filepath,
                 num_classes,
                 batch_size,
                 no_epochs):

  ''' Pipeline used to test the performance of the active learning algorithm'''
  
  # Loading in the tokenizer #
  transformers.logging.get_verbosity = lambda: logging.NOTSET
  tokenizer = BertTokenizerFast.from_pretrained(transformer_model)

  # Creating the dataset
  train = get_transformers_dataset(tokenizer, train_text, train_idx)
  test = get_transformers_dataset(tokenizer, test_text, test_idx)

  # Loading in the transformer model
  transformer_model = TransformerModelArguments(transformer_model)
  clf_factory = TransformerBasedClassificationFactory(transformer_model, 
                                                    num_classes, 
                                                    kwargs=dict({'device': 'cuda', 
                                                                 'mini_batch_size': batch_size,
                                                                 'early_stopping_no_improvement': -1, 
                                                                 'num_epochs':no_epochs
                                                                }))
  
  # Initialising the active learner
  active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)
  labeled_indices = initialize_active_learner(active_learner, train.y, numberofsamples=querysize)
  indices = []
  indices = indices + labeled_indices.tolist()
  results = []
  results.append(evaluate(active_learner, test))
  active_learners = {}
  texts = [train_text[i] for i in labeled_indices.tolist()]

  for i in range(numberofqueries):
    q_indices = active_learner.query(num_samples=querysize)

    # Simulate user interaction here. Replace this for real-world usage.
    y = train.y[q_indices]
    # Return the labels for the current query to the active learner.
    active_learner.update(y)
    text = [train_text[i] for i in q_indices]
    texts = texts + text
    print('Iteration #{:d} ({} samples)'.format(i, len(labeled_indices)))
    results.append(evaluate(active_learner, test))
    active_learners[i] =active_learner
    indices = indices + q_indices.tolist()

  df = pd.DataFrame()
  df['training_size'] = [querysize*(i+1) for i in range(numberofqueries + 1)]
  df['accuracy'] = [item[0] for item in results ]
  df['precision'] = [item[1] for item in results ]
  df['recall'] = [item[2] for item in results ]
  df['fscore'] = [item[3] for item in results ]
  df['model'] = [str(query_strategy) for item in results]
  df.to_csv(filepath + 'AL-results-{}.csv'.format(str(query_strategy)), escapechar = '\r')
  max_value = max(list(df['fscore']))
  best_performer = list(df['fscore']).index(max_value)
  save_active_leaner(active_learners[best_performer-1], filepath, best_performer, query_strategy)
  labelled_df = pd.DataFrame(columns = ['index','text', 'label'])
  labelled_df['index'] = indices
  labelled_df['text'] = texts
  labelled_df['labels'] = [train_idx[i] for i in indices]
  labelled_df.to_csv(filepath + "active_learning_labels.csv", escapechar = '\r')
  clear_session()
  return labelled_df, df



