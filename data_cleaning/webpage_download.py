#!/usr/bin/env python
# coding: utf-8

## Script to extract text from urls in social media posts ##

import pandas as pd
from pdfminer.layout import LAParams, LTTextBox
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfinterp import PDFResourceManager
from pdfminer.pdfinterp import PDFPageInterpreter
from pdfminer.converter import PDFPageAggregator
from pdfminer.converter import TextConverter
import io
from io import StringIO
import trafilatura
import numpy as np
import requests
import argparse
import os
from urllib.request import urlopen
import re
import requests
import timeout_decorator
import sys
import csv
from multiprocessing import  Pool

csv.field_size_limit(sys.maxsize)

def identify_urls(df, dedup=True) :
    
    ''' Code to create a new dataframe with just urls for twitter and facebook results'''

    newdf = pd.DataFrame(columns = list(df.columns)+ ['url'])
    for i in range(len(df)):
        newdf_2 = pd.DataFrame(columns = list(df.columns)+ ['url'])
        urllist = []
        if 'http' in str(df.loc[i, 'external_url']):
            urllist = urllist + str(df.loc[i, 'external_url']).split(",")
        if 'retweet_url' in df.columns:
            if 'http' in str(df.loc[i, 'retweet_url']):
                urllist = urllist + str(df.loc[i, 'retweet_url']).split(",")
        for j in range(len(urllist)):
            newdf_2.loc[j] = df.loc[i]
            newdf_2.loc[j, 'url'] = urllist[j]
        newdf = newdf.append(newdf_2)
        print("Resolved url row {}/{}".format(i+1, len(df)))
    newdf = newdf.drop(['external_url'], axis =1).reset_index(drop=True)
    for i in range(len(newdf)):
        if 'http' not in str(newdf.loc[i, 'url']) and 'www.' in str(newdf.loc[i, 'url']):
            newdf.loc[i, 'url'] = 'http://'+ str(newdf.loc[i, 'url'])
    droplist = [i for i in range(len(newdf)) if 'twitter.com' in str(newdf.loc[i, 'url'])]
    newdf = newdf.drop(droplist, axis=0).reset_index(drop=True)
    if dedup == True:
        return newdf.drop_duplicates(subset = ['url']).reset_index(drop=True)
    else:
        return newdf

def download_webpages(df):

    ''' Function to resolve and download all URLs in a dataframe'''

    for i in list(df.index):
        if 'bit.ly' in str(df.loc[i, 'url']):
            try:
                df.loc[i, 'url'] = resolve_url(str(df.loc[i, 'url']))
            except:
                print("couldn't resolve url: {}".format(df.loc[i, 'url']))
        if 't.co/' in str(df.loc[i, 'url']):
            try:
                df.loc[i, 'url'] = resolve_url(str(df.loc[i, 'url']))
            except:
                print("couldn't resolve url: {}".format(df.loc[i, 'url']))
        try:
            df.loc[i, 'url_text'] = extract_text(str(df.loc[i, 'url']))
            print("Extracted text from url {}/{}".format(i+1, len(df)))
        except:
            df.loc[i, 'url_text'] = np.nan
            print("could't extract text from {}".format(i))
    
    return df

@timeout_decorator.timeout(200) # Function times out after 200 seconds
def resolve_url(url):

    ''' Resolving Twitter URLs'''

    print("Resolving url {} =========>".format(url))
    return urlopen(url).geturl()


@timeout_decorator.timeout(300) # Function times out after 5 minutes
def extract_text(url):
    
    '''Function to extract text from urls'''
    
    if 'pdf' in url.lower(): # Downloading text from PDF files
        response_pdf = requests.get(url)
        pdf_stream = io.BytesIO(response_pdf.content)
        resource_manager = PDFResourceManager()
        fake_file_handle = io.StringIO()
        converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())
        page_interpreter = PDFPageInterpreter(resource_manager, converter)
        for page in PDFPage.get_pages(pdf_stream, caching=True, check_extractable=True):
            page_interpreter.process_page(page)
        text = fake_file_handle.getvalue()
        text = str(text)
        text = text.replace("\\n", "")
        text = re.sub(r"\(cid:.+\)", "", text)
        converter.close()
        fake_file_handle.close()
        return text
    
    else:
        url_data = requests.get(url)
        if url_data.status_code == 200:
            if len(str(url_data.content))<6000000: # Removing very large webpages
                text = trafilatura.extract(url_data.content, 
                                           include_images=False,
                                           include_formatting=False,
                                           include_comments=False)
                return text
            else:
                print("Couldn't download data from website (too long) {}".format(url))
        else:
            print("Couldn't download data from website (status_code) {}".format(url))

def parallelize_dataframe(df, func, n_cores=4):

    ''' Implementing parallel processing'''

    df_split = np.array_split(df, n_cores)
    pool = Pool(n_cores)
    df = pd.concat(pool.map(func, df_split))
    pool.close()
    pool.join()
    return df

def clean_data(df):

    ''' Removing blanks and duplicates'''

    print('Cleaning dataframe =========>')
    df = df[df['url_text'].apply(lambda x: len(str(x))>4)]
    df = df.drop_duplicates(subset = ['url']).reset_index(drop=True)
    return df

def main():

    ''' Main function that allows URLs to be extracted from all files in a directory or a single file'''

    parser = argparse.ArgumentParser()
    parser.add_argument("--df_directory", required = True, type=str, help='directory where files to be downloaded are stored')
    parser.add_argument("--num_processes", required =True, type=int, help='number of processes to run in parallel')
    parser.add_argument("--output_directory", required = True, type=str, help='directory where output files will be stored')
    parser.add_argument("--file", type=str, required = True, help='the file to extract text from - if "NA" then all files from the directory will be used')
    parser.add_argument('--url_id_string', type=str, required=True, help='string to add to url id to create new identifiers')
    args = parser.parse_args()
    if args.file == 'NA':
        dflist = os.listdir(args.df_directory)
    else:
        dflist = [args.file]
    for file in dflist:
        if ".csv" in file:
            df = pd.read_csv(args.df_directory + file, engine='python')
            if 'date' in df.columns:
                df['url_id'] = [args.url_id_string + str(df.loc[i, 'id'][0]) + str(df.loc[i, 'date'])[2:4] + str(i) for i in list(df.index)]
                df = identify_urls(df)
            else:
                df['url_id'] = [args.url_id_string + str(df.loc[i, 'id'][0]) + str(i) for i in list(df.index)]
                df = df.rename(columns={"external_url": "url"})       
            df_dedup = df.drop_duplicates(subset = ['url']).reset_index(drop=True)
            df2 = parallelize_dataframe(df_dedup, download_webpages, n_cores=args.num_processes)
            df2.to_csv(args.output_directory + file[:-4] + '_urltext.csv', escapechar='\r')
            clean_df = clean_data(df2)
            clean_df.to_csv(args.output_directory + file[:-4] + '_urltext_cleaned.csv', escapechar='\r')
            print("Extracted text from urls in file: {}".format(file))

if __name__ == "__main__":
    main()