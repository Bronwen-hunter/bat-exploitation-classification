#!/usr/bin/env python
# coding: utf-8

## Script used to deduplicate social media posts and articles ##

import pandas as pd
from itertools import combinations
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import csv
import argparse
import numpy as np

import sys
import PREPROCESS # This is available in the github repository
csv.field_size_limit(sys.maxsize)

def similarity_filter(dataframe, threshold, main_text):

  ''' Main function to remove duplicate texts '''

  dataframe = dataframe.reset_index(drop=True)
  pairs = combinations(list(dataframe.index), 2)
  vec = TfidfVectorizer()
  X = vec.fit_transform(list(dataframe[main_text]))
  titles_to_remove = []
  for pair in pairs:
    text1 = dataframe.loc[list(pair)[0], main_text]
    text2 = dataframe.loc[list(pair)[1], main_text]
    try:
        S = cosine_similarity(X[list(pair)[0]], X[list(pair)[1]])
        if S[0] > threshold:
            titles_to_remove.append(pair[0])
            print("removed: {}".format(text1))
    except:
        print("Couldn't assess pair {}".format(pair))

  return(dataframe.drop(titles_to_remove).reset_index(drop=True))

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--df_filepath', type=str, required=True, help='Filepath where results df is stored')
    parser.add_argument('--main_text', type=str, required=True, help='Text we are trying to deduplicated')
    parser.add_argument('--preprocessing', type=str, required=True, help='Do we want to preprocess the text?')
    parser.add_argument('--threshold', type=float, required=True, help='Similarity threshold')
    args = parser.parse_args()

    all_data1 = pd.read_csv(args.df_filepath)
    if args.preprocessing == 'True':
        all_data1[args.main_text] = [PREPROCESS.preprocess(text) for text in all_data1[args.main_text]]
    else:
        all_data1[args.main_text] =[text for text in all_data1[args.main_text]]

    listofdfs = np.array_split(all_data1, 500) # Splitting the full dataframe into 500 chunks (to avoid memory errors)
    for i, item in enumerate(listofdfs):
        listofdfs[i] =similarity_filter(item, args.threshold, args.main_text)
        print('Finished deduplicating dataframe {}'.format(i))
    all_data1_deduplicated = pd.concat(listofdfs)
    print('removed {} rows from the dataframe'.format(len(all_data1)-len(all_data1_deduplicated)))
    all_data1_deduplicated.to_csv(args.df_filepath[:-4] + '_deduplicated.csv', escapechar='\r')


if __name__ == "__main__":
    main()